{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reads in a cleaned corpus text file and returns the most frequent words and the most important sentences, representing a summary of the corpus.  The code is modeled after the code from the following paper by R. Mihalcea and P. Tarau:\n",
    "https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf\n",
    "\n",
    "Additional reference for implementing the paper's code:\n",
    "https://github.com/davidadamojr/TextRank/blob/master/textrank/__init__.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "\n",
    "import nltk, string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import textrank\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in file\n",
    "\n",
    "with open('exercise_benefits_corpus_cleaned.txt', 'r') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "corpus = corpus.decode('utf-8').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing:\n",
    "1. remove extra spaces, tabs, and returns\n",
    "2. stemming, lemmatisation, POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string, re\n",
    "\n",
    "def processed(text):\n",
    "    text = ' '.join(corpus.strip().split('\\n')).lower()\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    lem_text = lemmatiser.lemmatize(text)\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    lem_text = regex.sub('', lem_text)\n",
    "    return lem_text\n",
    "    \n",
    "clean = processed(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I compared stemming to lemmatizing and found that for this particular corpus, the two methods yielded very similar results.  I decided to go with lemmatizing since it avoids making up words, as stemming sometimes does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag text with POS (part of speech) & tokenize\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def tag_tokenize(processed_text):\n",
    "    tokens = word_tokenize(processed_text) # Generate list of tokens\n",
    "    tagged = pos_tag(tokens)\n",
    "    sentences = sent_tokenize(corpus)  #will use later in TextRank\n",
    "    return tagged\n",
    "\n",
    "tagged = tag_tokenize(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords & filter for tags:\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def filter_for_tags(tagged, tags=['NN', 'JJ', 'NNP']):\n",
    "    #filter based on POS tags\n",
    "    tagged = [item for item in tagged if item[1] in tags]\n",
    "    return tagged\n",
    "\n",
    "def filter_nostp(tagged_text):\n",
    "    filtered = filter_for_tags(tagged_text)\n",
    "    #filtered = re.sub(u\"\\u2019\", \"\", filtered)\n",
    "    stp = stopwords.words(\"english\")\n",
    "    add1 = [\"thats\", \"says\", \"theres\", \"its\", \"whats\", \"wheres\", \"even\", \"also\", \"may\", \"might\", \"think\", \"believe\", \"study\", \"dr\", \"university\"]\n",
    "    add = [unicode(i, \"utf-8\") for i in add1]\n",
    "    stop = stp + add\n",
    "    no_stp = [w[0] for w in filtered if w[0] not in stop]\n",
    "    return no_stp\n",
    "\n",
    "#will call the filter_nostp() function later in word frequency count\n",
    "\n",
    "tagged2 = filter_for_tags(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get most common words in corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exercise\n",
      "’\n",
      "new\n",
      "health\n",
      "training\n",
      "many\n",
      "“\n",
      "”\n",
      "time\n",
      "fat\n",
      "brain\n",
      "group\n",
      "activity\n",
      "week\n",
      "physical\n",
      "interval\n",
      "immune\n",
      "sedentary\n",
      "workout\n",
      "weight\n",
      "research\n",
      "blood\n",
      "mice\n",
      "fitness\n",
      "age\n",
      "memory\n",
      "heart\n",
      "highintensity\n",
      "science\n",
      "intense\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count = Counter(filter_nostp(tagged2))\n",
    "top30 = count.most_common(30)\n",
    "for i in top30:\n",
    "    print i[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank Algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize sentences\n",
    "sentences = sent_tokenize(corpus)\n",
    "\n",
    "def normalize(tagged):\n",
    "    \"\"\"Return a list of tuples with the first item's periods removed.\"\"\"\n",
    "    tagged = [(item[0].replace('.', ''), item[1]) for item in tagged]\n",
    "\n",
    "def unique_everseen(iterable, key=None):\n",
    "    #List unique elements in order of appearance.\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    if key is None:\n",
    "        for element in [x for x in iterable if x not in seen]:\n",
    "            seen_add(element)\n",
    "            yield element\n",
    "    else:\n",
    "        for element in iterable:\n",
    "            k = key(element)\n",
    "            if k not in seen:\n",
    "                seen_add(k)\n",
    "                yield element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x1a23c39550>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate cosine distance:\n",
    "import re, math\n",
    "WORD = re.compile(r'\\w+')\n",
    "import itertools\n",
    "import networkx as nx\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)    \n",
    "\n",
    "\n",
    "def build_graph(nodes):\n",
    "    \"\"\"Return a networkx graph instance.\n",
    "    :param nodes: List of hashables that represent the nodes of a graph.\n",
    "    \"\"\"\n",
    "    gr = nx.Graph()  # initialize an undirected graph\n",
    "    gr.add_nodes_from(nodes)\n",
    "    nodePairs = list(itertools.combinations(nodes, 2))\n",
    "\n",
    "    # add edges to the graph (weighted by cosine distance)\n",
    "    for pair in nodePairs:\n",
    "        firstString = pair[0]\n",
    "        vector1 = text_to_vector(firstString)\n",
    "        secondString = pair[1]\n",
    "        vector2 = text_to_vector(secondString)\n",
    "        pairwise_sim = get_cosine(vector1, vector2)\n",
    "        gr.add_edge(firstString, secondString, weight=pairwise_sim)\n",
    "\n",
    "    return gr\n",
    "\n",
    "build_graph(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return key phrases from corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_set = unique_everseen([x[0] for x in tagged2])\n",
    "word_set_list = list(unique_word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate score of each sentence\n",
    "\n",
    "calculated_page_rank = nx.pagerank(build_graph(sentences), weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important words in ascending order of importance\n",
    "keyphrases = sorted(calculated_page_rank, key=calculated_page_rank.get,\n",
    "                        reverse=True)\n",
    "\n",
    "# the number of keyphrases returned will be relative to the size of the\n",
    "# text (a third of the number of vertices)\n",
    "one_third = len(word_set_list) // 3\n",
    "keyphrases = keyphrases[0:one_third + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(0,19):\n",
    "    keyphrases[i] = keyphrases[i].encode('utf-8')\n",
    "    res.append(keyphrases[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She and her colleagues did find increases in the activity of certain genes and the levels of some proteins in the brains of the runners that could have contributed to the changes in their synapses, she says.\n",
      "These volunteers, who had been sedentary and overweight, were told they would be taking part in an exercise program to get them ready to complete a 5K race, and that the study would examine some of the effects of the training, including psychological impacts.\n",
      "When scientists in Sweden scanned the spines of mice before and after they ran for several weeks on treadmills, the researchers noticed significant increases in the size of their spinal discs, indicating that those structures had been responding and adapting to the demands of running.\n",
      "For their inaugural study of the riders , which was published in 2014, the scientists measured a broad range of the cyclists’ physical and cognitive abilities and compared them to those of sedentary older people and much younger men and women.\n",
      "In almost all of the volunteers, the fat tissue after exercise showed greater amounts of a protein that is known to contribute to the development of more blood vessels.\n",
      "Still, the study’s overall results suggest that even a few weeks of exercise can alter the makeup and function of people’s microbiomes, says Jeffrey Woods, a professor of kinesiology and community health at the University of Illinois who conducted the study, along with his doctoral student Jacob Allen (now a postdoctoral researcher at Ohio State University) and others.\n",
      "So for one of the new studies , the researchers turned to muscle tissue that already had been biopsied from the legs of 90 of the riders.\n",
      "Then, in perhaps the most important element of the study, the scientists waved farewell to their volunteers, but asked them to keep logs of their physical activities for a month, to see if people would choose to continue any of the workouts on their own.\n",
      "It results when the lower esophageal sphincter, a ring of muscle between the esophagus and the stomach, fails to close tightly enough to prevent the contents of the stomach from moving up instead of down.\n",
      "To no one’s surprise, when the scientists tallied results, the cyclists proved to have unanimously reported less fatigue and more happiness during the moderate pedaling, especially if the researchers checked in near the end of either of the sessions of tiring intervals.\n",
      "But afterward, with the exercise behind them, people’s opinions shifted and almost all of the volunteers decided that, in retrospect, they had found the workouts almost equally enjoyable and would in fact rank the standard, high-intensity interval training as the most pleasant.\n",
      "Beginning in the 1980s, a number of studies of marathon and ultramarathon runners had found that many of them reported developing colds in the days and weeks immediately after their race.\n",
      "These interval sessions continued three times a week for four months, which would approximate about eight years in our lives, according to Dr. Bruce Troen, a professor of medicine and head of the division of geriatrics at the University at Buffalo, who, with his colleague Kenneth Seldeen and others, conducted the study.\n",
      "So for the new review, in The Journal of Happiness Studies , researchers at the University of Michigan decided to aggregate and analyze multiple past studies of working out and happiness.\n",
      "So for the new study, which was published in May in Medicine & Science in Sports & Exercise , researchers from the University of British Columbia in Kelowna and McMaster University in Hamilton, Ontario, decided to ask a group of ordinary people to try a variety of workouts and see if they liked them.\n",
      "Since both greater size and increased levels of internal fluid indicate better disc health, the runners harbored fundamentally healthier spines than the people who were sedentary, says Daniel Belavy, the study leader and a professor of exercise at the Institute for Physical Activity and Nutrition at Deakin University.\n",
      "“So more muscle means more of that hormone,” says Janet Lord, the director of the Institute of Inflammation and Aging at the University of Birmingham, who was a co-author of both studies.\n",
      "That’s the conclusion of a fascinating new study in mice that found that 30 minutes on a treadmill affects gene activity within cardiac cells in ways that, over the long haul, could slow the aging of the animals’ hearts.\n",
      "The men and women in the group that had burned 1,500 calories a week with exercise proved to have compensated for nearly 950 of those calories, the numbers indicated.\n"
     ]
    }
   ],
   "source": [
    "#display neater output:\n",
    "\n",
    "for i in res:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open('exercise_benefits_summary.txt', 'w')\n",
    "\n",
    "for item in res:\n",
    "  textfile.write(\"%s\\n\" % item)\n",
    "textfile.close()\n",
    "\n",
    "#note: in the text file, all punctuation is back to normal (no weird symbols) as it is typically in UTF-8."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
