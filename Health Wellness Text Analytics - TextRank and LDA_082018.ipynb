{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reads in a cleaned corpus text file and returns the most frequent words and the most important sentences, representing a summary of the corpus.  The code is modeled after the code from the following paper by R. Mihalcea and P. Tarau:\n",
    "https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf\n",
    "\n",
    "Additional reference for implementing the paper's code:\n",
    "https://github.com/davidadamojr/TextRank/blob/master/textrank/__init__.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "\n",
    "import nltk, string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import textrank\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in file\n",
    "\n",
    "with open('genetic_testing_corpus_cleaned.txt', 'r') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "corpus = corpus.decode('utf-8').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing:\n",
    "1. remove extra spaces, tabs, and returns\n",
    "2. stemming, lemmatisation, POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string, re\n",
    "\n",
    "\n",
    "def processed(text):\n",
    "    text = ' '.join(corpus.strip().split('\\n')).lower()\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    lem_text = lemmatiser.lemmatize(text)\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    lem_text = regex.sub('', lem_text)\n",
    "    return lem_text\n",
    "    \n",
    "clean = processed(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I compared stemming to lemmatizing and found that for this particular corpus, the two methods yielded very similar results.  I decided to go with lemmatizing since it avoids making up words, as stemming sometimes does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag text with POS (part of speech) & tokenize\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def tag_tokenize(processed_text):\n",
    "    tokens = word_tokenize(processed_text) # Generate list of tokens\n",
    "    tagged = pos_tag(tokens)\n",
    "    sentences = sent_tokenize(corpus)  #will use later in TextRank\n",
    "    return tagged\n",
    "\n",
    "tagged = tag_tokenize(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords & filter for tags:\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def filter_for_tags(tagged, tags=['NN', 'JJ', 'NNP']):\n",
    "    #filter based on POS tags\n",
    "    tagged = [item for item in tagged if item[1] in tags]\n",
    "    return tagged\n",
    "\n",
    "def filter_nostp(tagged_text):\n",
    "    filtered = filter_for_tags(tagged_text)\n",
    "    #filtered = re.sub(u\"\\u2019\", \"\", filtered)\n",
    "    stp = stopwords.words(\"english\")\n",
    "    add1 = [\"thats\", \"says\", \"'\", \"theres\", \"-\", \"mr\", \"its\", \"whats\", \"wheres\", \"even\", \"also\", \"may\", \"might\", \"think\", \"believe\", \"study\", \"dr\", \"university\"]\n",
    "    add = [unicode(i, \"utf-8\") for i in add1]\n",
    "    stop = stp + add\n",
    "    no_stp = [w[0] for w in filtered if w[0] not in stop]\n",
    "    return no_stp\n",
    "\n",
    "#will call the filter_nostp() function later in word frequency count\n",
    "\n",
    "tagged2 = filter_for_tags(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get most common words in corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "’\n",
      "genetic\n",
      "”\n",
      "dna\n",
      "risk\n",
      "“\n",
      "new\n",
      "company\n",
      "disease\n",
      "test\n",
      "family\n",
      "ancestry\n",
      "medical\n",
      "percent\n",
      "health\n",
      "cancer\n",
      "gene\n",
      "—\n",
      "clement\n",
      "many\n",
      "european\n",
      "time\n",
      "information\n",
      "mother\n",
      "research\n",
      "intelligence\n",
      "african\n",
      "school\n",
      "last\n",
      "heart\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count = Counter(filter_nostp(tagged2))\n",
    "top30 = count.most_common(30)\n",
    "for i in top30:\n",
    "    print i[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank Algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize sentences\n",
    "sentences = sent_tokenize(corpus)\n",
    "\n",
    "def normalize(tagged):\n",
    "    \"\"\"Return a list of tuples with the first item's periods removed.\"\"\"\n",
    "    tagged = [(item[0].replace('.', ''), item[1]) for item in tagged]\n",
    "\n",
    "normalize(tagged2)    \n",
    "    \n",
    "def unique_everseen(iterable, key=None):\n",
    "    #List unique elements in order of appearance.\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    if key is None:\n",
    "        for element in [x for x in iterable if x not in seen]:\n",
    "            seen_add(element)\n",
    "            yield element\n",
    "    else:\n",
    "        for element in iterable:\n",
    "            k = key(element)\n",
    "            if k not in seen:\n",
    "                seen_add(k)\n",
    "                yield element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x1a3174c1d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate cosine distance:\n",
    "import re, math\n",
    "WORD = re.compile(r'\\w+')\n",
    "import itertools\n",
    "import networkx as nx\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)    \n",
    "\n",
    "\n",
    "def build_graph(nodes):\n",
    "    \"\"\"Return a networkx graph instance.\n",
    "    :param nodes: List of hashables that represent the nodes of a graph.\n",
    "    \"\"\"\n",
    "    gr = nx.Graph()  # initialize an undirected graph\n",
    "    gr.add_nodes_from(nodes)\n",
    "    nodePairs = list(itertools.combinations(nodes, 2))\n",
    "\n",
    "    # add edges to the graph (weighted by cosine distance)\n",
    "    for pair in nodePairs:\n",
    "        firstString = pair[0]\n",
    "        vector1 = text_to_vector(firstString)\n",
    "        secondString = pair[1]\n",
    "        vector2 = text_to_vector(secondString)\n",
    "        pairwise_sim = get_cosine(vector1, vector2)\n",
    "        gr.add_edge(firstString, secondString, weight=pairwise_sim)\n",
    "\n",
    "    return gr\n",
    "\n",
    "build_graph(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return key phrases from corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_set = unique_everseen([x[0] for x in tagged2])\n",
    "word_set_list = list(unique_word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate score of each sentence\n",
    "\n",
    "calculated_page_rank = nx.pagerank(build_graph(sentences), weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important words in ascending order of importance\n",
    "keyphrases = sorted(calculated_page_rank, key=calculated_page_rank.get,\n",
    "                        reverse=True)\n",
    "\n",
    "# the number of keyphrases returned will be relative to the size of the\n",
    "# text (a third of the number of vertices)\n",
    "one_third = len(word_set_list) // 3\n",
    "keyphrases = keyphrases[0:one_third + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(0,19):\n",
    "    keyphrases[i] = keyphrases[i].encode('utf-8')\n",
    "    res.append(keyphrases[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And with the financial support of a handful of like-minded wealthy individuals who agreed to invest in the exploratory phase of the project, “it just seemed,” Mr. Clement said, “like something I could do.” Even with the Harvard name as a calling card, several of the families he contacted over the next few years did not respond to his inquiries.\n",
      "A spokesperson for the agency declined comment, saying that “FTC investigations are non-public, and so typically we do not comment on an investigation or even whether we are investigating.” Privacy issues in the use of such DNA testing kits came to the forefront last month with the arrest of the notorious Golden State Killer, when it was revealed that police had used data from GEDMatch , a genealogy research site where users upload genealogical and genetic information, to help identify the suspect.\n",
      "“There is a demand.” Yet even if just a minority of 23andMe customers decided to game the current insurance system, “it’s enough to perturb the market,” said Dr. Robert Cook-Deegan, a professor at the school for the future of innovation in society at Arizona State University, who has studied the issue.\n",
      "Afterward, Dr. Theodora Ross, a geneticist who directs the high-risk cancer genetics program at the University of Texas Southwestern Medical Center, said she had a story for me: A recent patient of hers had sent a saliva sample to a direct-to-consumer testing company, 23andMe, which detects small differences in DNA at hundreds of locations that can indicate ancestry and various health conditions.For the most part, direct-to-consumer genetic tests are recreational genetics.\n",
      "If unusual patterns in their three billion pairs of A’s, C’s, G’s and T’s — the nucleobases that make up all genomes — can be shown to have prolonged their lives and protected their health, the logic goes, it is conceivable that a drug or gene therapy could be devised to replicate the effects in the rest of us.\n",
      "“It’s about the complexity of the connections between point A and B.” But the genetic links suggest another, perhaps stranger possibility: Some variants linked to education work not in the brains of students, but in the people they inherited the variants from — their parents.\n",
      "Talking to him, it was hard not to fantasize about the possibility that, as another Facebook participant suggested, “by the time we get to that age, we may all be living to 110.” At the time the oldest man in America, Mr. Matthews sometimes strained to hear, but his sense of humor and perspective were intact.\n",
      "The study began because there was general agreement among researchers that many common diseases are linked not to one mutation, but rather to thousands or millions of mutations, said the first author of the new paper, Dr. Amit V. Khera, a cardiologist at Massachusetts General Hospital and a researcher at the Broad Institute.\n",
      "The FTC probe appears to have been prompted by a letter from Sen. Chuck Schumer last November, in which the senate minority leader expressed concern that popular at-home DNA test kits could be putting consumer privacy at great risk : “Besides, putting your most personal genetic information in the hands of third parties for their exclusive use raises a lot of concerns, from the potential for discrimination by employers all the way to health insurance.\n",
      "“My preferred approach would be that this is done in the context of having somebody help you interpret that information, and talk to you about what it means in real time,” said Dr. Susan Domchek, a medical oncologist and expert on breast cancer genetics and prevention at Perelman School of Medicine at the University of Pennsylvania.\n",
      "Gretchen Ertl for The New York Times “My hat was off to someone who was willing to take the time out of his life to go get these precious specimens,” said Dr. Church, the Harvard geneticist, who has devoted a portion of his laboratory to research into the reversal of aging.\n",
      "It May Lurk in the DNA of the Oldest Among Us Clarence Matthews, 110, had blood drawn at his home in Indian Wells, Calif., last year, part of a project to examine the genes of the very old.\n",
      "Like all normal human genome sequences, the beginning of his first chromosome reads like this: TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCT Whether, in combination with the genomes of his fellow supercentenarians, the rest contains the secret to a long, healthy and happy life remains to be seen.\n",
      "Greg Lennon, a co-founder of Promethease, said that the company’s reports include a disclaimer saying the data are “not intended for medical or health purposes.” Customers are warned to seek out “an independent, clinically validated test” if they are told of a mutation and to seek out a genetic counselor.\n",
      "Until now, the only way for people to get such genetic tests was to see a medical professional who would order a test and later deliver the results to patients.\n",
      "It will include not just the genes for disease risk, but also reports of ancestry and results on things like how much the person is likely to weigh and whether alcohol will elicit flushing of the face, shoulders and neck, or even the entire body.\n",
      "But with a business plan that, even to some of his investors, sounded more like a research project, Mr. Clement seems to have undertaken the task largely because it provided the chance to act on a longstanding interest in human longevity, including his own.\n",
      "First, the Boston-based team combed previous studies that mapped the DNA of large numbers of people, looking for links to the five diseases — not outright mutations but minor misspellings in the genetic code.\n",
      "If I am true to myself and the scientific evidence that provides richness to the DNA I’ve inherited, I now need to figure out a way to honor all of me and those who survived to make me possible.\n"
     ]
    }
   ],
   "source": [
    "#display neater output:\n",
    "\n",
    "for i in res:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open('genetic_testing_summary.txt', 'w')\n",
    "\n",
    "for item in res:\n",
    "  textfile.write(\"%s\\n\" % item)\n",
    "textfile.close()\n",
    "\n",
    "#note: in the text file, all punctuation is back to normal (no weird symbols) as it is typically in UTF-8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Topic Modeling with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in texts\n",
    "\n",
    "articles = pd.read_csv(\"genetic_testing_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>He learned of his mixed-race ancestry through ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ms. Reilly found that she had inherited an Apo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Article                                               Text\n",
       "0        1  He learned of his mixed-race ancestry through ...\n",
       "1        2  Ms. Reilly found that she had inherited an Apo..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corp = articles.Text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stp = stopwords.words('english')\n",
    "add1 = [\"thats\", \"says\", \"'\", \"said\", \"people\", \"theres\", \"-\", \"mr\", \"its\", \"whats\", \"wheres\", \"even\", \"also\", \"may\", \"might\", \"think\", \"believe\", \"study\", \"dr\", \"university\"]\n",
    "add = [unicode(i, \"utf-8\") for i in add1]\n",
    "stop = stp + add\n",
    "stop = set(stop)\n",
    "\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    doc = doc.decode('utf-8').strip()\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "corp_clean = [clean(doc).split() for doc in corp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(corp_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in corp_clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = lda(doc_term_matrix, num_topics=10, id2word = dictionary, passes=20)\n",
    "ldamodel.save('model1.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.000*\"genetic\" + 0.000*\"test\" + 0.000*\"dna\" + 0.000*\"\\u2014\"')\n",
      "(1, u'0.000*\"mr\" + 0.000*\"dna\" + 0.000*\"\\u2014\" + 0.000*\"test\"')\n",
      "(2, u'0.025*\"gene\" + 0.021*\"intelligence\" + 0.018*\"study\" + 0.014*\"genetic\"')\n",
      "(3, u'0.012*\"dna\" + 0.009*\"family\" + 0.008*\"result\" + 0.008*\"ancestry\"')\n",
      "(4, u'0.021*\"company\" + 0.014*\"23andme\" + 0.013*\"genetic\" + 0.012*\"data\"')\n",
      "(5, u'0.025*\"risk\" + 0.020*\"genetic\" + 0.017*\"test\" + 0.017*\"cancer\"')\n",
      "(6, u'0.011*\"mr\" + 0.010*\"dna\" + 0.008*\"clement\" + 0.007*\"company\"')\n",
      "(7, u'0.000*\"genetic\" + 0.000*\"dna\" + 0.000*\"gene\" + 0.000*\"result\"')\n",
      "(8, u'0.013*\"dna\" + 0.012*\"percent\" + 0.011*\"family\" + 0.010*\"ancestry\"')\n",
      "(9, u'0.000*\"dna\" + 0.000*\"genetic\" + 0.000*\"family\" + 0.000*\"percent\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
